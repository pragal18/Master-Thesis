{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing sentence splitting for all the files in the context folder\n",
    "import os\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import rdflib\n",
    "tknzer=TweetTokenizer()\n",
    "\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "\n",
    "for filename in os.listdir('Context/'):\n",
    "    print(filename)\n",
    "    graph2=rdflib.Graph()\n",
    "    graph2.parse('Context/'+filename,format='nt')\n",
    "    g=Graph()\n",
    "    name=filename.split(\".\")[0]\n",
    "    s=graph2.serialize(format=\"nt\")\n",
    "    for s,p,o in graph2:\n",
    "        if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "            sentences = nltk.sent_tokenize(o)\n",
    "            for i in sentences:\n",
    "                try:\n",
    "                    BI=o.index(i)\n",
    "                    EI=o.index(i)+len(i)\n",
    "                    g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Sentence])\n",
    "                    g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "                    g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "                    g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(i)])\n",
    "                    g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=context\")])     \n",
    "                except:\n",
    "                    pass\n",
    "    g.bind(\"nif\",nif)        \n",
    "    #print(g.serialize(format=\"turtle\"))\n",
    "    print(g.serialize(destination='Sentencesplit/'+filename,format=\"turtle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing tokenization for all the files in a folder\n",
    "import os\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import rdflib\n",
    "tknzer=TweetTokenizer()\n",
    "\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "\n",
    "def spans(txt):\n",
    "    tokens=tknzer.tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "        \n",
    "for filename in os.listdir('Context/'):\n",
    "    print(filename)\n",
    "    graph2=rdflib.Graph()\n",
    "    graph2.parse('Context/'+filename,format='nt')\n",
    "    g=Graph()\n",
    "    name=filename.split(\".\")[0]\n",
    "    s=graph2.serialize(format=\"nt\")\n",
    "    for s,p,o in graph2:\n",
    "        if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "            sentences = nltk.sent_tokenize(o)\n",
    "            for i in range(len(sentences)):\n",
    "                count=0\n",
    "                try:\n",
    "                    BII=o.find(sentences[i])\n",
    "                    for token in spans(sentences[i]):\n",
    "                        assert token[0]==sentences[i][token[1]:token[2]]\n",
    "                        BI=BII+token[1]\n",
    "                        EI=BII+token[2]\n",
    "                        if token[0] not in string.punctuation:\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Word])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(token[0])])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=context\")])       \n",
    "                except:\n",
    "                    pass       \n",
    "    g.bind(\"nif\",nif)        \n",
    "    #print(g.serialize(format=\"turtle\"))\n",
    "    print(g.serialize(destination='Tokens2/'+filename,format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Part-of-speech tagging on all files in the Context folder\n",
    "import rdflib\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tknzr2 = RegexpTokenizer(r'\\w+')\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "tknzer=TweetTokenizer()\n",
    "\n",
    "def spans(txt):\n",
    "    tokens=nltk.word_tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "    \n",
    "data=pd.read_excel(\"pos-mapping2.xlsx\")\n",
    "for filename in os.listdir('Context/'):\n",
    "    print(filename)\n",
    "    name=filename.split(\".\")[0]\n",
    "    graph2=rdflib.Graph()\n",
    "    graph2.parse(\"Context/\"+filename,format='nt')\n",
    "    g=Graph()\n",
    "    for s,p,o in graph2:\n",
    "        if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "            sentences = nltk.sent_tokenize(o)\n",
    "            tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "            tagged = [nltk.pos_tag(sent) for sent in tokens] \n",
    "            for i in range(len(sentences)):\n",
    "                count=0\n",
    "                try:\n",
    "                    BII=o.find(sentences[i])\n",
    "                    for token in spans(sentences[i]):\n",
    "                        assert token[0]==sentences[i][token[1]:token[2]]\n",
    "                        BI=BII+token[1]\n",
    "                        EI=BII+token[2]\n",
    "                        #print(token[0]+\" \"+tagged[i][count][1])\n",
    "                        val=data.posfullform1[data.posshortcut1==tagged[i][count][1]]\n",
    "                        for jumbo in val:\n",
    "                            posfullform=jumbo\n",
    "                        hello=\"http://purl.org/olia/olia.owl#\"+ posfullform\n",
    "                        value=data.posfullformtype1[data.posshortcut1==tagged[i][count][1]]\n",
    "                        for jumbos in value:\n",
    "                            posfullformtype=jumbos\n",
    "                        hell=\"http://purl.org/olia/olia.owl#\"+ posfullformtype\n",
    "                        if token[0] not in string.punctuation:\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Word])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(token[0])])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=context\")])       \n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaLink,rdflib.term.URIRef(\"http://purl.org/olia/penn.owl#\"+tagged[i][count][1])])\n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaCategory,term.URIRef(hello)])                         \n",
    "                            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaCategory,term.URIRef(hell)])\n",
    "                        count=count+1\n",
    "                except:\n",
    "                    pass\n",
    "                               \n",
    "    g.bind(\"nif\",nif)        \n",
    "    #print(g.serialize(format=\"turtle\"))\n",
    "    print(g.serialize(destination=\"POS/\"+filename,format=\"turtle\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
