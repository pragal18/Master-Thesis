{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the number of lines in nif_context_en.ttl file obtained from https://wiki.dbpedia.org/dbpedia-nif-dataset\n",
    "num_lines = sum(1 for line in open('nif_context_en.ttl',encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the nif_context_en.ttl into individual files(~5 million files) where each file corresponds to one wikipedia page \n",
    "\n",
    "previousSubject = None\n",
    "file=open('nif_context_en.ttl',encoding=\"utf-8\")\n",
    "for i in range(29642397):\n",
    "    fileline=file.readline()\n",
    "    s=fileline.find('?')\n",
    "    thisSubject=fileline[29:s]  \n",
    "    if previousSubject==thisSubject:\n",
    "      f.write(fileline);\n",
    "    else :\n",
    "      try:  \n",
    "        f=open(\"Contents/\"+thisSubject+\".ttl\",'a',encoding=\"utf-8\")\n",
    "        f.write(fileline);\n",
    "      except:\n",
    "        pass\n"
    "    previousSubject=thisSubject"
   ]
  },
  {
   "cell_type": "code",
   "execut"ion_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the triples for one indiviual file named Animalia_(book).ttl\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import nltk\n",
    "g = Graph()\n",
    "g.parse('Animalia_(book).ttl', format='nt')\n",
    "for stmt in g:\n",
    "    print(stmt)\n",
    "for s,p,o in g:\n",
    "    print(s,p,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence splitting,tokenisation and part of speech for one individual file named Animalia_(book) \n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import nltk\n",
    "g = Graph()\n",
    "g.parse('Animalia_(book).ttl', format='nt')\n",
    "for s, p, o in g:\n",
    "    if type(o) == rdflib.term.Literal:             \n",
    "        sentences = nltk.sent_tokenize(o-)\n",
    "        tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        print(sentences)\n",
    "        #print(tokens)\n",
    "        tagged = [nltk.pos_tag(sent) for sent in tokens] \n",
    "        #print(tagged)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the result of sentence splitting in triples for an individual file named Animalia_(book)\n",
    "#Please refer the Animalia_(book)-sentencesplit.ttl for the output\n",
    "import rdflib\n",
    "import nltk\n",
    "from rdflib import Graph,term\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "#nif=rdflib.term.URIRef('http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#')\n",
    "\n",
    "graph2=rdflib.Graph()\n",
    "name=\"Animalia_(book)\"\n",
    "graph2.parse(name+'.ttl',format='nt')\n",
    "g=Graph()\n",
    "s=graph2.serialize(format=\"nt\")\n",
    "count=0\n",
    "namespace_manager = NamespaceManager(Graph())\n",
    "namespace_manager.bind('ns1', nif, override=True)\n",
    "\n",
    "for s,p,o in graph2:\n",
    "    if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "        sentences = nltk.sent_tokenize(o)\n",
    "        tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        tokens2= nltk.word_tokenize(o)\n",
    "        for i in sentences:\n",
    "            try:\n",
    "                BI=o.index(i)\n",
    "                EI=o.index(i)+len(i)\n",
    "            except:\n",
    "                pass\n",
    "            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Sentence])\n",
    "            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(i)])\n",
    "            g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=sentence_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/Animalia_(book)?dbpv=2016-10&nif=context\")])\n",
    "    \n",
    "g.bind(\"nif\",nif)  \n",
    "print(g.serialize(format=\"turtle\"))\n",
    "print(g.serialize(destination=name+\"sentencesplit.ttl\",format=\"turtle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the result of tokenization in triples for an individual file named Animalia_(book)\n",
    "#Please refer the Animalia_(book)-tokenization.ttl for the output\n",
    "import rdflib\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "tknzr2 = RegexpTokenizer(r'\\w+')\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "#nif=rdflib.term.URIRef('http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#')\n",
    "tknzer=TweetTokenizer()\n",
    "graph2=rdflib.Graph()\n",
    "name=\"Animalia_(book)\"\n",
    "graph2.parse(name+'.ttl',format='nt')\n",
    "g=Graph()\n",
    "s=graph2.serialize(format=\"nt\")\n",
    "def spans(txt):\n",
    "    tokens=nltk.word_tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "    \n",
    "\n",
    "for s,p,o in graph2:\n",
    "    if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "        sentences = nltk.sent_tokenize(o)\n",
    "        tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        tagged = [nltk.pos_tag(sent) for sent in tokens] \n",
    "        for i in range(len(sentences)):\n",
    "                #print(word)\n",
    "                count=0\n",
    "                try:\n",
    "                    print(sentences[i])\n",
    "                    BII=o.find(sentences[i])\n",
    "                    for token in spans(sentences[i]):\n",
    "                        \n",
    "                        #print(token[0])\n",
    "                        print(tagged[i][count][1])\n",
    "                        assert token[0]==sentences[i][token[1]:token[2]]\n",
    "                        BI=BII+token[1]\n",
    "                        EI=BII+token[2]\n",
    "                        #print(str(BI)+str(EI))\n",
    "                        \n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Word])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(token[0])])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/Animalia_(book)?dbpv=2016-10&nif=context\")])       \n",
    "                        count=count+1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "               \n",
    "g.bind(\"nif\",nif)        \n",
    "print(g.serialize(format=\"turtle\"))\n",
    "print(g.serialize(destination=name+\"-tokenization.ttl\",format=\"turtle\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the result of part-of-speech in triples for an individual file named Animalia_(book)\n",
    "#Please refer the Animalia_(book)-pos.ttl for the output\n",
    "import rdflib\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tknzr2 = RegexpTokenizer(r'\\w+')\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "tknzer=TweetTokenizer()\n",
    "graph2=rdflib.Graph()\n",
    "name=\"Animalia_(book)\"\n",
    "graph2.parse(name+'.ttl',format='nt')\n",
    "g=Graph()\n",
    "s=graph2.serialize(format=\"nt\")\n",
    "def spans(txt):\n",
    "    tokens=nltk.word_tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "    \n",
    "data=pd.read_excel(\"pos-mapping2.xlsx\")\n",
    "for s,p,o in graph2:\n",
    "    if type(o)==rdflib.term.Literal and nif.isString in p:\n",
    "        sentences = nltk.sent_tokenize(o)\n",
    "        tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        tagged = [nltk.pos_tag(sent) for sent in tokens] \n",
    "        for i in range(len(sentences)):\n",
    "                count=0\n",
    "                try:\n",
    "                    BII=o.find(sentences[i])\n",
    "                    for token in spans(sentences[i]):\n",
    "                        assert token[0]==sentences[i][token[1]:token[2]]\n",
    "                        BI=BII+token[1]\n",
    "                        EI=BII+token[2]\n",
    "                        val=data.posfullform1[data.posshortcut1==tagged[i][count][1]]\n",
    "                        for jumbo in val:\n",
    "                            posfullform=jumbo\n",
    "                        hello=\"http://purl.org/olia/olia.owl#\"+ posfullform\n",
    "                        value=data.posfullformtype1[data.posshortcut1==tagged[i][count][1]]\n",
    "                        for jumbos in value:\n",
    "                            posfullformtype=jumbos\n",
    "                        hell=\"http://purl.org/olia/olia.owl#\"+ posfullformtype\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),RDF.type,nif.Word])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.beginIndex,rdflib.term.Literal(str(BI))])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.endIndex,rdflib.term.Literal(str(EI))])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.anchorOf,rdflib.term.Literal(token[0])])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.referenceContext,rdflib.term.URIRef(\"http://dbpedia.org/resource/Animalia_(book)?dbpv=2016-10&nif=context\")])       \n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaLink,rdflib.term.URIRef(\"http://purl.org/olia/penn.owl#\"+tagged[i][count][1])])\n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaCategory,term.URIRef(hello)])                         \n",
    "                        g.add([rdflib.term.URIRef(\"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BI)+\"_\"+str(EI)),nif.oliaCategory,term.URIRef(hell)])\n",
    "                        count=count+1\n",
    "                except:\n",
    "                    pass\n",
    "                               \n",
    "g.bind(\"nif\",nif)        \n",
    "#print(g.serialize(format=\"turtle\"))\n",
    "print(g.serialize(destination=name+\"-pos.ttl\",format=\"turtle\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the surface forms within a single document in a wikipedia page(surface_forms-pos-link)\n",
    "import rdflib\n",
    "import nltk\n",
    "from rdflib import Graph,term,Literal,XSD\n",
    "from rdflib.namespace import RDFS,RDF, FOAF, NamespaceManager\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import xlwt\n",
    "tknzr2 = RegexpTokenizer(r'\\w+')\n",
    "nif=rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "tknzer=TweetTokenizer()\n",
    "graph2=rdflib.Graph()\n",
    "name=\"Animalia_(book)\"\n",
    "graph2.parse(name+'-links.ttl',format='nt')\n",
    "g=Graph()\n",
    "g.parse(name+'.ttl',format='nt')\n",
    "graph1.parse(name+'-pos3.ttl',format='ttl')\n",
    "book=xlwt.Workbook(encoding='utf-8')\n",
    "sheet1 = book.add_sheet(\"Sheet 1\")\n",
    "sheet1.write(0,0,\"Surface forms\")\n",
    "sheet1.write(0,1,\"POS\")\n",
    "sheet1.write(0,2,\"Links\")\n",
    "row=1\n",
    "\n",
    "for s,p,o in graph2:\n",
    "    print(s)\n",
    "    BIEI=s.split(\"char=\",1)[1]\n",
    "    #print(s)\n",
    "    BI=BIEI.split(\",\",1)[0]\n",
    "    EI=BIEI.split(\",\",1)[1]\n",
    "    print(BI+\" \"+EI)\n",
    "    \n",
    "    for sub,pred,obj in g:\n",
    "        \n",
    "        if type(obj)==rdflib.term.Literal and nif.isString in pred:\n",
    "            #print(obj)\n",
    "            word=obj[int(BI):int(EI)]\n",
    "            \n",
    "            #worksheet.write(row,col,word)\n",
    "            sheet1.write(row,0,word)\n",
    "            sheet1.write(row,2,\"https://en.wikipedia.org/wiki/\"+word.replace(\" \",\"_\"))\n",
    "            print(word)\n",
    "            wordcount=0\n",
    "            for i in word.split(\" \"):\n",
    "                \n",
    "                BII=int(BI)\n",
    "                #print(BII)\n",
    "                EII=int(BII)+len(i)\n",
    "                BI=int(EII)+1\n",
    "                print(str(BII)+\" \"+str(EII))\n",
    "                for su,pr,ob in graph1:\n",
    "                        if \"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BII)+\"_\"+str(EII) in su and \"phrase\" not in s:\n",
    "                            if nif.oliaLink in pr:\n",
    "                                print(ob.split(\"#\")[1])\n",
    "                                sheet1.write(row,1,ob.split(\"#\")[1])\n",
    "                        if \"http://dbpedia.org/resource/\"+name+\"?dbpv=2016-10&nif=word_\"+str(BII)+\"_\"+str(EII) in su and \"phrase\" in s:\n",
    "                            if nif.oliaLink in pr and wordcount==1:    \n",
    "                                print(ob.split(\"#\")[1])\n",
    "                                sheet1.write(row,1,ob.split(\"#\")[1])\n",
    "                           \n",
    "                print(\" \") \n",
    "                wordcount=wordcount+1\n",
    "            row=row+1   \n",
    "\n",
    "book.save(name+'surfaceforms.xls')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
